{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "320eacc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pprint\n",
    "segment_questions_list = []\n",
    "path = r'C:\\\\Users\\\\maganti\\\\Documents\\\\INFO 5502 Chen Sir Project UNT\\\\Web Scraping Interviews\\\\Narrators Json Files-Updatedwithquestions'\n",
    "for filename in glob.glob(os.path.join(path, '*.json')): #only process .JSON files in folder.     \n",
    "    #print(filename)\n",
    "    with open(filename, mode = 'rb') as currentFile:\n",
    "        data = json.loads(currentFile.read())\n",
    "        try:\n",
    "            if data['transcript'] != {}:\n",
    "                for segment in data[\"transcript\"].keys():\n",
    "                    for question in data[\"transcript\"][segment]['Segment Questions']:\n",
    "                        segment_questions_list.append(' '.join(i for i in question.split()[1:]))\n",
    "                        #print(' '.join(i for i in question.split()[1:]))\n",
    "                        #print(data['transcript'][segment]['Segment Questions'][question])\n",
    "                        \n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "print(len(segment_questions_list))\n",
    "print()\n",
    "\n",
    "\n",
    "#print(' '.join(i for i in segment_questions_list if i != ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f445c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "#text = 'hello bye the the hi'\n",
    "# doc = ' '.join([word for title in segment_title_list for word in title.split() if word not in (stopwords.words('english'))])\n",
    "doc = \" \".join(question for question in segment_questions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6663483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2125593"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dd857c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mightn', 'me', 'is', 'why', 'so', \"they're\", 'would', 'to', 'ought', \"they'll\", 'myself', 'such', 'each', \"it's\", \"you'll\", \"wasn't\", 'there', \"wouldn't\", 'those', 'my', 'our', \"we're\", 'than', 'any', \"what's\", 'these', 'herself', 'mustn', 'shan', 'of', \"doesn't\", 'over', 'cannot', 'in', 'some', 'haven', 'could', 'hasn', 'isn', 'hadn', 'with', 'against', 'he', 'most', 'themselves', \"that's\", 'what', 'i', 'your', 'couldn', 'can', 'just', 'having', 'up', 'how', 'ain', \"mightn't\", \"can't\", \"she's\", 'and', 'yourselves', 'now', 'll', 'doing', 'the', \"he's\", 'as', 'been', 'doesn', 'do', \"i'm\", \"there's\", 'itself', 'wasn', 'd', \"who's\", 'if', 'yours', 'which', 'own', 'further', \"we've\", 'below', 'after', \"here's\", 'until', \"aren't\", 'once', 'very', 'this', \"he'd\", 's', 'ma', \"i'll\", 'whom', 'needn', 'between', \"shan't\", \"i'd\", 'has', 'ourselves', \"she'd\", 'nor', 'was', \"haven't\", \"couldn't\", 'm', 'should', 'we', \"won't\", 'am', 'here', 'they', 'through', 'be', 'weren', 'for', \"you've\", 'you', 'their', \"they'd\", 'not', 'yourself', 'out', \"isn't\", 'shouldn', 'are', \"you're\", \"i've\", 'both', 'ours', \"why's\", 'have', 'a', \"mustn't\", \"hadn't\", 'on', 't', 'too', \"he'll\", 'where', \"when's\", \"shouldn't\", 'hers', 'won', \"where's\", 'before', 'under', 'her', 'few', 'aren', 'other', 'theirs', 'had', 'at', \"how's\", 're', 'from', \"we'd\", 've', 'when', \"needn't\", 'o', \"we'll\", 'his', 'it', 'about', \"should've\", 'didn', 'no', 'all', 'y', \"didn't\", 'while', 'into', 'who', 'same', 'by', 'because', 'wouldn', 'himself', 'were', 'did', 'during', 'she', 'its', \"you'd\", 'them', 'him', 'does', 'again', \"hasn't\", \"weren't\", 'will', 'or', 'that', \"they've\", 'more', \"don't\", \"she'll\", 'off', 'only', 'an', \"let's\", 'don', 'then', 'being', 'but', \"that'll\", 'down', 'above'}\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "211\n"
     ]
    }
   ],
   "source": [
    "doc = doc.lower()\n",
    "import re\n",
    "doc = re.sub('[^A-Za-z0-9 ]', '', doc)\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "stop_words = list(get_stop_words('en'))\n",
    "\n",
    "print((set(list(STOPWORDS)) | set(list(stop_words))))\n",
    "stopwords = list(set(list(STOPWORDS)) | set(list(stop_words)))\n",
    "print(type(list(STOPWORDS)))\n",
    "print(type((stop_words)))\n",
    "print(len(stopwords))\n",
    "stopwords = [re.sub('[^a-zA-Z0-9]+', '', word) for word in stopwords]\n",
    "doc = \" \".join(word for word in doc.split() if word not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca8324f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007472"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd4a6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_doc = \" \".join(wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in doc.split())\n",
    "\n",
    "noun_lemmatized_doc = \" \".join(wordnet_lemmatizer.lemmatize(word, pos=\"n\") for word in lemmatized_doc.split())\n",
    "adj_lemmatized_doc = \" \".join(wordnet_lemmatizer.lemmatize(word, pos=\"a\") for word in noun_lemmatized_doc.split())\n",
    "adv_lemmatized_doc = \" \".join(wordnet_lemmatizer.lemmatize(word, pos=\"r\") for word in adj_lemmatized_doc.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33e190e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "allWords = nltk.tokenize.word_tokenize(adv_lemmatized_doc)\n",
    "allWordDist = nltk.FreqDist(w.lower() for w in allWords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "062b2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_from_segment_questions = {}\n",
    "for common_word in list(allWordDist):\n",
    "    #print(common_word, allWordDist[common_word])\n",
    "    common_words_from_segment_questions[common_word] = allWordDist[common_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f742271",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Common_words_from_Questions.json\", \"w\") as outfile:\n",
    "    json.dump(common_words_from_segment_questions, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de708395",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open (r'C:\\\\Users\\\\maganti\\\\Documents\\\\INFO 5502 Chen Sir Project UNT\\\\Key Words Extraction\\\\Yingying_edited_stopwords\\\\YingyingEdit_Common_words_from_Questions_1600.json', \"r\")\n",
    "common_words_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42f8e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_words_data\n",
    "for word in common_words_data.keys():\n",
    "    if common_words_data[word] > 1600:\n",
    "        stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea81cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_doc = \" \".join(word for word in adv_lemmatized_doc.split() if word not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08a101f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7128644\n",
      "12467573\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_doc))\n",
    "print(len(adv_lemmatized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "660898d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngram - 1\n",
      "[('internmenttype', 0.329), ('kakas', 0.3065), ('honorables', 0.3034), ('cadet', 0.303), ('internment', 0.2984), ('amnesty', 0.2976), ('okadasans', 0.2886), ('kokka', 0.2876), ('rotc', 0.2823), ('correctional', 0.2819), ('okadasan', 0.2817), ('corpus', 0.28), ('sergeant', 0.2799), ('shift', 0.2782), ('checklist', 0.2777), ('consulars', 0.275), ('kapa', 0.2745), ('kuboyama', 0.2739), ('intermarry', 0.2735), ('familyll', 0.2735), ('inokuchis', 0.2731), ('sgt', 0.2717), ('ichiokas', 0.2706), ('idiq', 0.2697), ('hive', 0.2695), ('bikur', 0.269), ('koden', 0.2689), ('honorable', 0.2685), ('kohara', 0.2675), ('mailman', 0.2662), ('gang', 0.266), ('kakka', 0.266), ('aapa', 0.2655), ('kois', 0.2654), ('memo', 0.2653), ('kotoka', 0.2649), ('kokikoko', 0.2642), ('recruitment', 0.2637), ('kokuryukai', 0.2636), ('inokuchi', 0.2635)]\n",
      "\n",
      "Ngram - 2\n",
      "[('roster internment', 0.4), ('discrimination join', 0.3818), ('internment police', 0.38), ('internment join', 0.3784), ('internment check', 0.3736), ('column brother', 0.3724), ('assimilate intermarry', 0.3653), ('inflow immigrant', 0.365), ('join shift', 0.3633), ('discrimination sergeant', 0.3627), ('internment add', 0.3619), ('incident join', 0.36), ('job battalion', 0.3595), ('internment ccc', 0.354), ('internment spell', 0.3537), ('incident office', 0.3532), ('internment supreme', 0.3524), ('sergeant interrogate', 0.3506), ('minority gang', 0.3506), ('consider gang', 0.35), ('challenge internment', 0.3486), ('battalion kaki', 0.3481), ('sergeant join', 0.3473), ('ethnicity office', 0.3465), ('arrest join', 0.3463), ('internment prison', 0.346), ('officer list', 0.3456), ('internment civil', 0.3454), ('sergeant full', 0.3451), ('internment occur', 0.3451), ('internment marry', 0.3448), ('hall officer', 0.3447), ('hall sergeant', 0.3446), ('shift brother', 0.3441), ('internment challenge', 0.3441), ('internment shikata', 0.3437), ('column unfortunately', 0.3433), ('internment correspond', 0.3429), ('sergeant list', 0.3425), ('segregation senior', 0.3424)]\n",
      "\n",
      "Ngram - 3\n",
      "[('locate roster internment', 0.4145), ('internment case roster', 0.4114), ('roster internment', 0.4), ('roster internment especially', 0.393), ('continue inflow immigrant', 0.3902), ('internment order join', 0.3861), ('internment record sheet', 0.3835), ('internment police sometimes', 0.3824), ('interrogate top sergeant', 0.3822), ('internment minidoka join', 0.3821), ('discrimination join', 0.3818), ('inflow immigrant cut', 0.3813), ('internment police', 0.38), ('internment challenge task', 0.3787), ('internment join join', 0.3784), ('internment join', 0.3784), ('satisfy join rotc', 0.3782), ('internment honor roll', 0.3782), ('office immigration district', 0.3779), ('internment opportunity reed', 0.3774), ('prison puzzle internment', 0.3767), ('join gang type', 0.3762), ('hall invite internment', 0.3762), ('floor family kawada', 0.3761), ('internment family join', 0.3754), ('inflow immigrant community', 0.3737), ('internment check', 0.3736), ('unit ko family', 0.373), ('internment check 20000', 0.3727), ('column brother', 0.3724), ('rot job office', 0.3722), ('family internment prison', 0.37), ('office aiko kawakami', 0.3688), ('street emi kuboyama', 0.3677), ('job husband battalion', 0.3672), ('challenge internment evidence', 0.3667), ('file clerk brother', 0.3656), ('assimilate intermarry', 0.3653), ('roster sheet parole', 0.3651), ('inflow immigrant', 0.365)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "for i in range(1,4):\n",
    "    print(\"Ngram - {}\".format(i))\n",
    "    keywords = kw_model.extract_keywords(cleaned_doc, keyphrase_ngram_range=(1, i), top_n=40, stop_words=None)\n",
    "    print(keywords)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7b0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
