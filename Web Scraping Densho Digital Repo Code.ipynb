{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import HTTPError\n",
    "import json\n",
    "import re\n",
    "total_count = 0\n",
    "count = 0\n",
    "main_url = \"https://ddr.densho.org/narrators/?page={}\"\n",
    "for page_num in range(1, 38):\n",
    "\n",
    "    link1 = Request(main_url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    url1 = urlopen(link1)\n",
    "\n",
    "    data1 = url1.read()\n",
    "    data1_soup = BeautifulSoup(data1)\n",
    "\n",
    "    print(\"*** Page {} ***\".format(page_num))\n",
    "    for narrator_link in data1_soup.find_all('h4'):\n",
    "        #print(narrator_link.a.get('href'))\n",
    "        link2 = Request(narrator_link.a.get('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        #print(narrator_link.a.get('href'))\n",
    "        url2 = urlopen(link2)\n",
    "\n",
    "        data2 = url2.read()\n",
    "        data2_soup = BeautifulSoup(data2)\n",
    "        count = count + 1\n",
    "        \n",
    "        #print(\"*** Interview links page***\")\n",
    "        narrator = data2_soup.find_all(\"div\", attrs={'class':'col-sm-8 col-md-8'})[0]\n",
    "        if data2_soup.find_all(\"div\", attrs={'class':'url'}) != []:\n",
    "            interview_count = 1\n",
    "            for interviews in data2_soup.find_all(\"div\", attrs={'class':'url'}):\n",
    "                #interview_count = 1\n",
    "                result = {}\n",
    "                result['Narrator_Name'] = \"\"\n",
    "                result['Bio'] = \"\"\n",
    "                result['Title'] = \"\"\n",
    "                result['Interviewer'] = \"\"\n",
    "                result['Location'] = \"\"\n",
    "                result['Date'] = \"\"\n",
    "                result['Densho ID'] = \"\"\n",
    "                result['transcript'] = {}\n",
    "                #print(interviews.a.get('href'))\n",
    "                #print(\"Interview\")\n",
    "                #print(interviews.a.get('href'))\n",
    "                # Narrator name and Bio...\n",
    "                try:\n",
    "                    result['Narrator_Name'] = narrator.h1.text.strip().replace('\"', \"\")#re.sub('\"', '\\\"', narrator.h1.text.strip())\n",
    "                except:\n",
    "                    result['Narrator_Name'] = \"\"\n",
    "                    \n",
    "                try:\n",
    "                    result['Bio'] = narrator.p.text\n",
    "                except :\n",
    "                    result['Bio'] = \"\"\n",
    "                \n",
    "                link3 = Request(interviews.a.get('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                #print(interviews.a.get('href'))\n",
    "                url3 = urlopen(link3)\n",
    "\n",
    "                data3 = url3.read()\n",
    "                data3_soup = BeautifulSoup(data3)\n",
    "                #count = 1\n",
    "                #print(\"*** Segment Descriptions ***\")\n",
    "                \n",
    "                num = 1\n",
    "                for desc in data3_soup.find_all('tbody')[0].find_all('td'):\n",
    "                    result['transcript']['segment {}'.format(num)] = {}\n",
    "                    result['transcript']['segment {}'.format(num)]['Segment Desc'] = desc.text.split(' (', 1)[0].strip()\n",
    "                    result['transcript']['segment {}'.format(num)]['Segment Questions'] = []\n",
    "                    num = num + 1\n",
    "                    \n",
    "                for transcript_link in data3_soup.find_all('div', attrs={'id':'dls_tab'}):\n",
    "                    print(\"*** Transcript link ***\")\n",
    "                    try:\n",
    "                        print(transcript_link.find_all('a')[1].get('href'))\n",
    "                        link4 = Request(transcript_link.find_all('a')[1].get('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                        url4 = urlopen(link4)\n",
    "\n",
    "                        data4 = url4.read()\n",
    "                        data4_soup = BeautifulSoup(data4)\n",
    "\n",
    "                        #print(\"-----------------------------------------\")\n",
    "                        for data in data4_soup.find_all('div', attrs={'class':'segmentHead'}):\n",
    "                            # title, interviewer, location, date, densho id...\n",
    "                            for attr in data.text.splitlines():\n",
    "                                if 'Title' in attr:\n",
    "                                    #print(i.strip())\n",
    "                                    result['Title'] = ' '.join(attr.strip().split()[1:])\n",
    "                                if 'Interviewer' in attr:\n",
    "                                    #print(i.strip())\n",
    "                                    result['Interviewer'] = ' '.join(attr.strip().split()[1:])\n",
    "                                if 'Location' in attr:\n",
    "                                    #print(i.strip())\n",
    "                                    result['Location'] = ' '.join(attr.strip().split()[1:])\n",
    "                                if 'Date' in attr:\n",
    "                                    #print(i.strip())\n",
    "                                    result['Date'] = ' '.join(attr.strip().split()[1:])\n",
    "                                if 'Densho ID' in attr:\n",
    "                                    #print(i.strip())\n",
    "                                    result['Densho ID'] = ' '.join(attr.strip().split()[2:])\n",
    "                        \n",
    "                        num = 1\n",
    "\n",
    "                        for seg in data4_soup.find_all('div', attrs={'class':'segmentBody'}):\n",
    "                            #print(\"*** Segment Body ***\")\n",
    "                            #result['transcript']['segment {}'.format(num)]['Segment Desc'] = \"\"\n",
    "                            #result['transcript']['segment {}'.format(num)]['Segment Questions'] = re.sub(\"   +\", \"\", \"\\n\".join(p.text.strip() for p in seg.find_all('p')[1:-1]))\n",
    "                            for p in seg.find_all('p')[1:-1]:\n",
    "                                if '?' in p.text:\n",
    "                                    #print(re.sub(\"  +\", \"\",p.text))\n",
    "                                    result['transcript']['segment {}'.format(num)]['Segment Questions'].append(re.sub('\\s+',' ', re.sub(\"  +\", \"\",p.text)))\n",
    "\n",
    "                            '''\n",
    "                            for p in seg.find_all('p')[1:-1]:\n",
    "                                print(\" \".join(p.text.strip().split()))\n",
    "                            '''   \n",
    "                            num = num + 1\n",
    "                     \n",
    "                    #except HTTPError as err:\n",
    "                    except Exception as e:\n",
    "                        print(\"No Transcripts\")\n",
    "                        print(e)\n",
    "                        print(len(data3_soup.find_all('tbody')[0].find_all('td')))\n",
    "                        result['Title'] = \"\"\n",
    "                        result['Interviewer'] = \"\"\n",
    "                        result['Location'] = \"\"\n",
    "                        result['Date'] = \"\"\n",
    "                        result['Densho ID'] = \"\"\n",
    "                        #num = 1\n",
    "                        result['transcript'] = {}\n",
    "                        for num in range(0, len(data3_soup.find_all('tbody')[0].find_all('td'))):\n",
    "                            result['transcript']['segment {}'.format(num+1)] = {}\n",
    "                            result['transcript']['segment {}'.format(num+1)]['Segment Desc'] = \"\"\n",
    "                            result['transcript']['segment {}'.format(num+1)]['Segment Questions'] = []\n",
    "                            #num = num + 1\n",
    "                                                   \n",
    "                        \n",
    "\n",
    "                #print()\n",
    "                print(result)\n",
    "                import os.path\n",
    "                if os.path.isfile(r'C:\\\\Users\\\\maganti\\\\Documents\\\\INFO 5502 Chen Sir Project UNT\\\\Web Scraping Interviews\\\\Narrators Json Files-Updatedwithquestions\\\\{}.json'.format(result['Narrator_Name'] + '-' + str(interview_count))):\n",
    "                    with open(r'C:\\\\Users\\\\maganti\\\\Documents\\\\INFO 5502 Chen Sir Project UNT\\\\Web Scraping Interviews\\\\Narrators Json Files-Updatedwithquestions\\\\{}.json'.format(result['Narrator_Name'] + '(1)' + '-' + str(interview_count)), 'w', encoding='utf-8') as f:\n",
    "                        json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "                else:\n",
    "                    with open(r'C:\\\\Users\\\\maganti\\\\Documents\\\\INFO 5502 Chen Sir Project UNT\\\\Web Scraping Interviews\\\\Narrators Json Files-Updatedwithquestions\\\\{}.json'.format(result['Narrator_Name'] + '-' + str(interview_count)), 'w', encoding='utf-8') as f:\n",
    "                        json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "                print('***************************')\n",
    "                interview_count = interview_count + 1 \n",
    "             \n",
    "        else:\n",
    "            '''\n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "            print(\"No Interviews....\")\n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "            '''\n",
    "            result= {}\n",
    "            result['Narrator_Name'] = narrator.h1.text.strip().replace('\"', '')#re.sub('\"', '\\\"', narrator.h1.text.strip())\n",
    "            result['Bio'] = narrator.p.text\n",
    "            result['Title'] = \"\"\n",
    "            result['Interviewer'] = \"\"\n",
    "            result['Location'] = \"\"\n",
    "            result['Date'] = \"\"\n",
    "            result['Densho ID'] = \"\"\n",
    "            result['transcript'] = {}\n",
    "            print(result)\n",
    "            with open(r'C:\\\\Users\\\\maganti\\\\Documents\\\\INFO 5502 Chen Sir Project UNT\\\\Web Scraping Interviews\\\\Narrators Json Files-Updatedwithquestions\\\\{}.json'.format(result['Narrator_Name']), 'w', encoding='utf-8') as f:\n",
    "                json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "            #interview_count = interview_count + 1\n",
    "            print(\"****************************\")\n",
    "    #break\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Total count of narrators is: {}\".format(count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
